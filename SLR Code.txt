# Read in the data.
# This data represents the Forced Exhalation Volume (FEV) collected on people 3 to 19 years of age.
# Note that the "SLRRed.txt" contains a reduced data set (2 observations per age group) to illustrate the difference
# in the Frequentist and Bayesian lines.
alldat = read.table("C:\\Users\\shuej\\OneDrive\\Desktop\\Work\\Stat5370\\Stat5370 - Spr21\\SLRRed.txt", header=T)

# Uncomment the line below to see the full data set, and examine how closely the Frequentist and Bayesian methods
# align for large sample size.
# alldat = read.table("C:\\Users\\shuej\\OneDrive\\Desktop\\Work\\Stat5370\\Stat5370 - Spr21\\SLRFull.txt", header=T)


# To make life easy, give the variables simple names, x and y.
x = alldat$age
y = alldat$FEV
plot(x,y)

# You'll notice that the variability in y increases as x increases.  So try log transformations.
y = log(alldat$FEV)
x = log(alldat$age)
plot(x,y, main="log(FEV) vs. log(age)", xlab="log(age)", ylab="log(FEV)")

# Now that this looks much better, we will run a linear regression on these transformed variables.
# Note that this complicates our interpretation of our results because the analysis is done in the
# log-scale of the FEV.  Further details on diagnosing and fixing problems like these are left
# for a course on regression/linear models.

# Some basic variables used throughout.
n = length(y)
p = 1

# Build y.
y = matrix(y,nrow=n,ncol=1)

# Build X.
X = matrix(nrow=n,ncol=2)
X[,1] = rep(1,n)
X[,2] = x

# Note that in R, A%*%B is matrix multiplication, t(A) is transpose, and the inverse of A is found by solve(A).

# Frequentist analysis.
# Find betahat.
XpXinv = solve(t(X)%*%X) # Since we use this a lot, give it a name.
betahat = XpXinv %*% t(X) %*% y

# Find MSE
MSE = 1/(n-(p+1)) * t(y - X%*%betahat) %*% (y - X%*%betahat)
MSE = as.numeric(MSE) # Make it a scalar.

# Find estimated variance/covariance for betahat.
fvcovbeta = MSE * XpXinv

# Plot regression line, prediction intervals, confidence intervals.
x.plot = seq(min(x), max(x), 0.1)
y.plot = betahat[1] + betahat[2]*x.plot
lines(x.plot, y.plot, lty=1, col="black")
legend("topleft", legend=c("Freq. Regression Line", "Freq. CI/PI", "Bay. Regression Line", "Bay. HPD/PI"), col=c("black", "black", "red", "red"), lty=c(1,2,1,2))

ftcrit = qt(0.975,df=n-(p+1))

lci = vector(mode="numeric", length=length(x.plot))
uci = vector(mode="numeric", length=length(x.plot))
lpi = vector(mode="numeric", length=length(x.plot))
upi = vector(mode="numeric", length=length(x.plot))
for (i in 1:length(x.plot))
{
  x.star = matrix(c(1,x.plot[i]), nrow=1, ncol=2)
  lci[i] = as.numeric(x.star%*%betahat - ftcrit*sqrt(MSE)*sqrt(x.star%*%XpXinv%*%t(x.star)))
  uci[i] = as.numeric(x.star%*%betahat + ftcrit*sqrt(MSE)*sqrt(x.star%*%XpXinv%*%t(x.star)))
  lpi[i] = as.numeric(x.star%*%betahat - ftcrit*sqrt(MSE)*sqrt(1+x.star%*%XpXinv%*%t(x.star)))
  upi[i] = as.numeric(x.star%*%betahat + ftcrit*sqrt(MSE)*sqrt(1+x.star%*%XpXinv%*%t(x.star)))
}

lines(x.plot, lci, lty=2, col="black")
lines(x.plot, uci, lty=2, col="black")
lines(x.plot, lpi, lty=2, col="black")
lines(x.plot, upi, lty=2, col="black")

# Bayesian Analysis
# Prior parameters.  Note that I have zero expertise with FEV, so these are just pulled out of thin air.
# Someone actually doing this regression would (hopefully) have more expertise and have better prior
# knowledge about the relationship between FEV and age.

b0 = matrix(ncol=1,nrow=2)
b0[1,1] = 0  # beta0 prior mean
b0[2,1] = 0  # beta1 prior mean

B0 = matrix(ncol=2,nrow=2)
# All of these descriptors are valid after multiplying by sigma^2
B0[1,1] = 1   # beta0 prior variance
B0[2,2] = 0.5 # beta1 prior variance
B0[1,2] = 0.2 # Prior covariance between beta0 and beta1
B0[2,1] = B0[1,2] # Prior covariance... matrix must be symmetric.

n0 = 10 # Prior "sample size"
S0 = 0.1  # Prior "expected value" (approximately) of sigma^2

# Find posterior parameters.
B1 = solve(solve(B0)+t(X)%*%X)
b1 = B1 %*% (solve(B0)%*%b0+t(X)%*%y)
n1 = n0 + n
S1 = as.numeric((n0*S0 + (t(b0)%*%solve(B0)%*%b0 + t(y)%*%y - t(b1)%*%solve(B1)%*%b1))/n1)

# Plot Bayesian stuff now.  Note that I'm reusing the variables here, but they aren't the same meaning.
x.plot = seq(min(x), max(x), 0.1)
y.plot = b1[1] + b1[2]*x.plot
lines(x.plot, y.plot, lty=1, col="red")

btcrit = qt(0.975,df=n1)
lci = vector(mode="numeric", length=length(x.plot))
uci = vector(mode="numeric", length=length(x.plot))
lpi = vector(mode="numeric", length=length(x.plot))
upi = vector(mode="numeric", length=length(x.plot))
for (i in 1:length(x.plot))
{
  x.star = matrix(c(1,x.plot[i]), nrow=1, ncol=2)
  lci[i] = as.numeric(x.star%*%b1 - btcrit*sqrt(S1)*sqrt(x.star%*%B1%*%t(x.star)))
  uci[i] = as.numeric(x.star%*%b1 + btcrit*sqrt(S1)*sqrt(x.star%*%B1%*%t(x.star)))
  lpi[i] = as.numeric(x.star%*%b1 - btcrit*sqrt(S1)*sqrt(1+x.star%*%B1%*%t(x.star)))
  upi[i] = as.numeric(x.star%*%b1 + btcrit*sqrt(S1)*sqrt(1+x.star%*%B1%*%t(x.star)))
}

lines(x.plot, lci, lty=2, col="red")
lines(x.plot, uci, lty=2, col="red")
lines(x.plot, lpi, lty=2, col="red")
lines(x.plot, upi, lty=2, col="red")

# Output results.
cat("beta0hat = ", betahat[1], " (se = ", sqrt(fvcovbeta[1,1]), "), 95% CI = (", betahat[1]-ftcrit*sqrt(fvcovbeta[1,1]), ", ", betahat[1]+ftcrit*sqrt(fvcovbeta[1,1]), ")\n", sep="")
cat("beta1hat = ", betahat[2], " (se = ", sqrt(fvcovbeta[2,2]), "), 95% CI = (", betahat[2]-ftcrit*sqrt(fvcovbeta[2,2]), ", ", betahat[2]+ftcrit*sqrt(fvcovbeta[2,2]), ")\n", sep="")
cat("MSE = ", MSE, "\n", sep="")

# Posterior variance/covariance for beta.
bvcovbeta = S1*B1

cat("Posterior mean of beta0 = ", b1[1], " (sd = ", sqrt(bvcovbeta[1,1]), "), 95% HPD = (", b1[1]-btcrit*sqrt(bvcovbeta[1,1]), ", ", b1[1]+btcrit*sqrt(bvcovbeta[1,1]), ")\n", sep="")
cat("Posterior mean of beta1 = ", b1[2], " (sd = ", sqrt(bvcovbeta[2,2]), "), 95% HPD = (", b1[2]-btcrit*sqrt(bvcovbeta[2,2]), ", ", b1[2]+btcrit*sqrt(bvcovbeta[2,2]), ")\n", sep="")
cat("Posterior mean of sigma^2 (approx) = ", S1, "\n", sep="")



